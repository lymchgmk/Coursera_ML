모멘텀 또는 기울기 하강와 모멘텀 이라고 하는
알고리즘이 있는데요, 일반적인 기울기 하강 알고리즘보다 거의 항상
더 빨리 작동합니다. 한 문장으로 말하자면, 기본 아이디어는
기울기의 기하급수적 가중 평균치를 산출하는 것인데요 그 다음에, 이 기울기를 이용해서 weights를
업데이트 하는 것입니다. 이번 비디오에서는, 한 문장으로 표현한 이 설명을
풀어서 해석하고 어떻게 도입하는지 한번 보겠습니다. 하나의 예로, 이런 곡선을 가진 비용함수를 최적화시키려고 한다고 해봅시다. 빨간점은 최소점의 위치를 나타냅니다. 아마 여기서 기울기 강하를 시작할 수도 있는데요,
1번의 iteration을 기울기 강하나 미니 배치 기울기 강하 의 경우
이곳으로 향하게 됩니다. 하지만 이제는 타원에서 반대편에 있게 되는데요, 그리고 또 다른 기울기 강하의 단계를 거치면
이렇게 하게 됩니다. 또 추가 단계, 그 다음 단계 등등 말이죠. 이렇게 해서 기울기 강하가 여러차례를 거치게 되는 것을 보실텐데요. 그쵸? 천천히 최소값을 향해 왔다갔다하면서 접근하게 됩니다. 그리고 이러한 왔다갔다하는 변동폭이 기울기 강하를
느리게 합니다. 그리하여 더 큰 러닝속도를 쓰지 않게 하고 말이죠. 특히, 만약 훨씬 더 큰 러닝속도를 쓰는 경우엔, 결과적으로 오버슈팅을하거나 갈라지게 할 수 있습니다. 그렇기 때문에 변동이 너무 크게 왔다갔다 하지 않게 하려는 것은 러닝 속도를 너무 큰 값을 사용하지 않게 강요할 것입니다. 이 문제를 보는 또 다른 시각은 바로 세로축에서 입니다. 이 경웨 러닝속도가 조금 늦길 바랄텐데요,
이 변동이 싫기 때문이죠. 하지만 가로축의 경우에는 빠른 러닝을 원할 것입니다. 그 이유는 더 공격적으로 왼쪽에서 오른쪽으로 이동하길 바라기 때문입니다. 저기 빨간색 점인 최소값을 향해 말이죠. 그러면 여러분이 기울기 강하 와 모멘텀을 도입할때
할 수 있는 것들입니다. iteration 마다, 더 상세하게, iteration t마다, 기본적인 derivative dw, db를 산출할 것입니다. 저는 위첨자 대괄호 L은 생략하겠습니다. 결국 현재의 미니 배치 에서 dw와 db를 계산하는 것입니다. 그리고 여러분이 배치 기울기 강하를 쓰는 경우, 현재 미니 배치를 그냥 전체 배치 일 것입니다. 그리고 이것은 배치 기울기 강하의 경우 잘 작동합니다. 그러므로 지금 현재 여러분의 미니 배치 가
전체 트레이닝세트인 경우, 이것도 잘 작동합니다. 그러면 그 다음으로는 vdW를 베터 vdw 더하기 1 빼기 베타 dW를 산출합니다. 이것은 이전에 계산했던 쎄타 = 베타 v 쎄타 더하기 1 빼기
베타 쎄타 t입니다. 즉, w의 derivative에 대해 가중 평균치를 구하는 것인데요, 다음으로, 비슷하게 vdb = 이것 더하기 1 빼기 베타 곱하기 db로 산출합니다. 그러고나서 weight를 업데이트할텐데요, w는 w 빼기 러닝속도 곱하기 ,
dW로 업데이트 하는 대신에, derivative와 말이죠, 대신에 dvW로 업데이트 합니다. 그리고 비슷하게 b는 b 빼기 알파 곱하기 vdb로 합니다. 그러면 이것이 하는 것은
기울기 강하의 절차를 스무스하게 해주는 것입니다. 예를 들어, 마지막 몇개의 derivative에서
산출한게 이렇게 됐다고 해봅시다. 이거, 이거, 이거, 이거입니다. 이 gradient의 평균을 구하면,
세로축의 방향으로 변동은 거의 0과 가깝게 나올 것입니다. 그러므로 세로 방향으로
더 느리게 하고 싶을텐데요, 이것이 양수와 음수를 밸런스시켜
평균이 거의 0과 가깝게 될 것입니다. 반면에 가로축 방향에서는 모든 derivative가 가로 방량의 오른쪽을
가르키는데요, 그러므로 가로방향에서의 평균은
아직 꽤 큰 값일 것입니다. 그렇기 때문에 여기 몇개의 iteration을 따른 알고리즘에서 기울기 강하 와 모멘텀 이 결과적으로 세로방향에서 훨씬 더 작은 변동을 나타내는 것을 찾을 수 있습니다. 하지만 단순히 더 빨리 가로 방향으로 움직이는 것에 초점이 맞춰져 있죠. 그러므로 이것이 여러분의 알고리즘이
더 단순한 여정을 밟도록 해줍니다. 또는 최소값의 방향에서 변동이 무뎌지게 해주거나 말이죠. 간혹 몇몇 분들에 대해서 한가지 직관적인 부분이 이해가 되는 설명이 있는데요, 여러분이 만약에
밥그릇 모양의 함수를 최소화한다고 생각하면, 이것은 실제로 밥그릇의 곡선입니다. 저는 그림은 잘 못 그리는 것 같아요. 그럼 그릇 모양의 함수를 최소화 시키는데요, 이런 derivative 값은 아랫막길로 내려가고 있는 공에 악셀을 제공한다고 생각하면 됩니다. 여기 모멘텀 용어들은 속돌를 대표한다고 생각하면 됩니다. 여러분이 그릇이 있다고 가정하고,
공을 갖고 악셀의 일부를 이 공에 부여하는 개념인데요, 이 공이 내릿막길에서 내려오고 있다고 생각해보겠습니다. 그러면 더욱더 악셀에 따라 빨리 구르겠죠. 그리고 베타는 1보다 약간 작은 값이 되기 때문에, 마찰력이 약간 적용되어 공이 제한없이 속도가 붙는 것을
막아줍니다. 그러므로 기울기 강하가 이전의 단계를 각각 개인별로 취하는 것이 아닌 이제는 이 작은 공이 아랫막길에서
모멘텀을 얻어 굴러 내려갈 수 있습니다. 여기 이 그릇과 같이 생긴데서
모멘텀을 얻어서 말이죠. 여기 이렇게 그릇에서 내려간다는 비유가 물리의 직관을 이용하는 사람에게는 잘 이해된다고 생각합니다. 하지만 모든사람이 이해를 쉽게 할 수 있는 것은 아니겠죠. 이 부분이 잘 이해가 되지 않으시더라고 걱정하지 마십시요. 마지막으로 이것을 어떻게 도입하는지 보겠습니다. 이것이 알고리즘인데요. 이제 러닝속도 알파와 여기 parameter 베타와 같이
2개의 하이퍼 파라미터가 있습니다. 이것들은 여러분의 기하급수적 가중평균을 조정하죠. 베타의 가장 흔한 값은 0.9입니다. 그리고 저희는 지금 10일간의 평균기온을 산출하는 것인데요. 지난 10개의 기울기 강하의 평균치를 구하는 것입니다. 실제로는 베타가 0.9인 것이 잘 작동합니다. 여러분은 얼마든지 다른 값도 시도해보셔도 되는데요, 그렇게 하여 하이퍼 파라미터 서치를 해보세요,
그렇지만 0.9의 베타값이 꽤 잘 작동하는 값입니다. 그럼, 바이어스 보정도 해 볼까요, 그렇죠? vdW 와 vdb 를 갖고, 나누기 1 빼기 베타의 t승을 해줍니다. 실제로 사람들은 이것을 잘 사용 안 하는데요, 그 이유는 단지 10개의 iteration 이류,
가중평균치는 이미 워밍업이 되어서 더 이상 bias estimate이 더 이상
아니기 때문입니다. 그렇기 때문에 실제로 많은 사람들이
바이어스 보정에 대해 신경쓰는 것을 보지 못합니다. 기울기 강하나 모멘텀을 도입할 때 말이죠. 그리고 당연히 이 절차는 vdW를 0으로 초기화시켜주죠. 아시겠지만 여기서 매트릭스는 0으로 이루어진 매트릭스로
dW와 동일한 다이메션을 갖습니다. dW는 W와 또 동일한 차원으로 이루어져 있고요. 그리고 Vdb 또한 0의 벡터로 초기회되죠. 그러면 db와 같은 차원,
결과적으로 b와 같은 차원을 갖게 됩니다. 마지막으로 여러분이 기울기 강하 와 모멘텀에 대한 학술을 읽으시면,
이 항이 생략되는 것을 보실텐데요 1 빼기 베타 부분말이죠. 그렇게해서 vdW = 베타 vdw 더하기 dW로 남게됩니다. 여기 보라색 버전을 쓰면서 나타나는 최종 효과는
vdW가 1 빼기 1베타로 스케일 된다는 것입니다.
또는, 더 정확히 얘기하면 1 나누기 1-베타로 말입니다. 그러므로 여러분이 기울기 강하 갱신을 할때
알파의 값은 그에 상응하는 1 나누기 1 빼기 베타로 변해야 합니다. 실제로 이 2개 모두 잘 작동할텐데요. 러닝속도 알파가 가장 최적의 값이
어떤것이지에 대해 영향을 줍니다. 저는 개인적으로 이 공식이 조금 덜 직관적으로 이해가 되는데요. 그 이유는 이것의 영향이 바로
하이퍼 파라미터 베타를 튜닝하는 경우, 이것이 vdW 와 vdb의 스케일링에도 영향을 주기 때문입니다. 그러면 결과적으로 러닝속도 알파를 어쩌면
가시 튜닝해야할 수도 있습니다. 그렇기 때문에 저는 개인적으로
여기 왼쪽에 적은 공식을 선호합니다. 1 빼기 베타를 생략하는 것보다 말이죠. 저는 그래서 왼쪽의 공식을 이용하는 편인데요, 여기 1 빼시 베타가 포함된 것 말이죠. 하지만 베타의 값이 0.9인 2개 모두
하이퍼 파라미터로써 흔한 초이스입니다. 단지 러닝속도가 이 2가지 버전에서 다른 방식으로 튜닝되야 한다는 점이 있는 것뿐이죠. 이것이 그럼 기울기 강하 와 모멘텀에 대한
내용의 전부인데요, 이것이 거의 항상 모멘텀이 없는 그냥 기울기 강하 알고리즘보다
더 잘 작동할 것입니다. 하지만 여러분의 러닝 알고리즘을 빨라지게
만드는 다른 방법이 또 있는데요, 다음 이어지는 비디오에서 이런 내용을 다루겠습니다.